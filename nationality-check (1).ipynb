{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8231738,"sourceType":"datasetVersion","datasetId":4881805}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nationality Detection Model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-07-15T08:40:49.840789Z","iopub.execute_input":"2025-07-15T08:40:49.842272Z","iopub.status.idle":"2025-07-15T08:40:49.850806Z","shell.execute_reply.started":"2025-07-15T08:40:49.842221Z","shell.execute_reply":"2025-07-15T08:40:49.849748Z"}}},{"cell_type":"markdown","source":"**Import the Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom tqdm import tqdm\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport cv2\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:44:29.784577Z","iopub.execute_input":"2025-07-16T11:44:29.785303Z","iopub.status.idle":"2025-07-16T11:44:29.791530Z","shell.execute_reply.started":"2025-07-16T11:44:29.785280Z","shell.execute_reply":"2025-07-16T11:44:29.790679Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"**Find out the path and read the CSV file**","metadata":{}},{"cell_type":"code","source":"# list all .csv files and show the full path\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            print(os.path.join(dirname, filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:44:35.499477Z","iopub.execute_input":"2025-07-16T11:44:35.499798Z","iopub.status.idle":"2025-07-16T11:46:15.751140Z","shell.execute_reply.started":"2025-07-16T11:44:35.499776Z","shell.execute_reply":"2025-07-16T11:46:15.750486Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fairface/fairface/fairface_label_val.csv\n/kaggle/input/fairface/fairface/fairface_label_train.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ✅ Correct file paths for train and validation CSVs\ntrain_csv = '/kaggle/input/fairface/fairface/fairface_label_train.csv'\nval_csv = '/kaggle/input/fairface/fairface/fairface_label_val.csv'\n\n# Load the data\ndf_train = pd.read_csv(train_csv)\ndf_val = pd.read_csv(val_csv)\n\n# Preview the training data\nprint(df_train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:46:48.455772Z","iopub.execute_input":"2025-07-16T11:46:48.456058Z","iopub.status.idle":"2025-07-16T11:46:48.561633Z","shell.execute_reply.started":"2025-07-16T11:46:48.456040Z","shell.execute_reply":"2025-07-16T11:46:48.560813Z"}},"outputs":[{"name":"stdout","text":"          file    age  gender        race  service_test\n0  train/1.jpg  50-59    Male  East Asian          True\n1  train/2.jpg  30-39  Female      Indian         False\n2  train/3.jpg    3-9  Female       Black         False\n3  train/4.jpg  20-29  Female      Indian          True\n4  train/5.jpg  20-29  Female      Indian          True\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Ensures the Dataset Healt and Summary**","metadata":{}},{"cell_type":"code","source":"# Checkinf that how many samples exist for every race label (our target)\nprint(\"Race Label Distribution in Training Data:\")\nprint(df_train['race'].value_counts())\n\n# Checking that if any missing values across the columns\nprint(\"\\n Missing Values in Training Data:\")\nprint(df_train.isnull().sum())\n\n# Also, check that the how many number of samples are present in dataset\nprint(\"\\n Total Training Samples:\", len(df_train))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:46:56.191632Z","iopub.execute_input":"2025-07-16T11:46:56.191913Z","iopub.status.idle":"2025-07-16T11:46:56.220957Z","shell.execute_reply.started":"2025-07-16T11:46:56.191892Z","shell.execute_reply":"2025-07-16T11:46:56.220013Z"}},"outputs":[{"name":"stdout","text":"Race Label Distribution in Training Data:\nrace\nWhite              16527\nLatino_Hispanic    13367\nIndian             12319\nEast Asian         12287\nBlack              12233\nSoutheast Asian    10795\nMiddle Eastern      9216\nName: count, dtype: int64\n\n Missing Values in Training Data:\nfile            0\nage             0\ngender          0\nrace            0\nservice_test    0\ndtype: int64\n\n Total Training Samples: 86744\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Remove service test rows which true\ndf_train = df_train[df_train['service_test'] == False].reset_index(drop=True)\ndf_val = df_val[df_val['service_test'] == False].reset_index(drop=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:47:07.762057Z","iopub.execute_input":"2025-07-16T11:47:07.762626Z","iopub.status.idle":"2025-07-16T11:47:07.772128Z","shell.execute_reply.started":"2025-07-16T11:47:07.762604Z","shell.execute_reply":"2025-07-16T11:47:07.771535Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Converting the Race Column value in Numeric form for model understanding**","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf_train['race_label'] = label_encoder.fit_transform(df_train['race'])\ndf_val['race_label'] = label_encoder.transform(df_val['race'])\n\n# Show the encoded class mapping\nrace_classes = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"Encoded Race Classes:\")\nprint(race_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:47:14.113609Z","iopub.execute_input":"2025-07-16T11:47:14.113921Z","iopub.status.idle":"2025-07-16T11:47:14.129656Z","shell.execute_reply.started":"2025-07-16T11:47:14.113903Z","shell.execute_reply":"2025-07-16T11:47:14.128843Z"}},"outputs":[{"name":"stdout","text":"Encoded Race Classes:\n{'Black': 0, 'East Asian': 1, 'Indian': 2, 'Latino_Hispanic': 3, 'Middle Eastern': 4, 'Southeast Asian': 5, 'White': 6}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Load and Pair Images with Labels","metadata":{}},{"cell_type":"code","source":"# ✅ Corrected image directories\ntrain_img_dir = \"/kaggle/input/fairface/fairface/train\"\nval_img_dir = \"/kaggle/input/fairface/fairface/val\"\n\n# ✅ Fix img_path columns accordingly\ndf_train['img_path'] = df_train['file'].apply(lambda x: os.path.join(train_img_dir, os.path.basename(x)))\ndf_val['img_path'] = df_val['file'].apply(lambda x: os.path.join(val_img_dir, os.path.basename(x)))\n\n# Double-check\ndf_train[['file', 'img_path']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:50:42.232008Z","iopub.execute_input":"2025-07-16T11:50:42.232562Z","iopub.status.idle":"2025-07-16T11:50:42.315227Z","shell.execute_reply.started":"2025-07-16T11:50:42.232539Z","shell.execute_reply":"2025-07-16T11:50:42.314371Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"           file                                      img_path\n0   train/2.jpg   /kaggle/input/fairface/fairface/train/2.jpg\n1   train/3.jpg   /kaggle/input/fairface/fairface/train/3.jpg\n2   train/7.jpg   /kaggle/input/fairface/fairface/train/7.jpg\n3  train/10.jpg  /kaggle/input/fairface/fairface/train/10.jpg\n4  train/12.jpg  /kaggle/input/fairface/fairface/train/12.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train/2.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/2.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train/3.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/3.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train/7.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/7.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train/10.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/10.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train/12.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/12.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"Converting Categorical value into numeric value","metadata":{}},{"cell_type":"code","source":"# 🧠 Label Encoding: Convert race (string labels) to numeric labels\n\nlabel_encoder = LabelEncoder()\n\n# Fit on training set and transform both train and validation sets\ndf_train['race_encoded'] = label_encoder.fit_transform(df_train['race'])\ndf_val['race_encoded'] = label_encoder.transform(df_val['race'])  # Use same encoder\n\n# 🖨️ Display mapping and a sample\nprint(\"Race Label Mapping:\")\nfor i, label in enumerate(label_encoder.classes_):\n    print(f\"{i}: {label}\")\n\nprint(\"\\nSample Encoded Labels:\")\nprint(df_train[['race', 'race_encoded']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:51:22.532559Z","iopub.execute_input":"2025-07-16T11:51:22.532851Z","iopub.status.idle":"2025-07-16T11:51:22.550681Z","shell.execute_reply.started":"2025-07-16T11:51:22.532830Z","shell.execute_reply":"2025-07-16T11:51:22.549752Z"}},"outputs":[{"name":"stdout","text":"Race Label Mapping:\n0: Black\n1: East Asian\n2: Indian\n3: Latino_Hispanic\n4: Middle Eastern\n5: Southeast Asian\n6: White\n\nSample Encoded Labels:\n             race  race_encoded\n0          Indian             2\n1           Black             0\n2  Middle Eastern             4\n3  Middle Eastern             4\n4      East Asian             1\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk(\"/kaggle/input/fairface\"):\n    print(dirname)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:48:50.052862Z","iopub.execute_input":"2025-07-16T11:48:50.053680Z","iopub.status.idle":"2025-07-16T11:49:20.843194Z","shell.execute_reply.started":"2025-07-16T11:48:50.053652Z","shell.execute_reply":"2025-07-16T11:49:20.842431Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fairface\n/kaggle/input/fairface/fairface\n/kaggle/input/fairface/fairface/val\n/kaggle/input/fairface/fairface/train\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\n\n# ✅ Show files inside the correct train directory\nfor filename in os.listdir(\"/kaggle/input/fairface/fairface/train\")[:10]:  # limit to 10\n    print(filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:56:00.531515Z","iopub.execute_input":"2025-07-16T11:56:00.531829Z","iopub.status.idle":"2025-07-16T11:56:00.961010Z","shell.execute_reply.started":"2025-07-16T11:56:00.531808Z","shell.execute_reply":"2025-07-16T11:56:00.960218Z"}},"outputs":[{"name":"stdout","text":"64601.jpg\n31973.jpg\n30778.jpg\n19812.jpg\n22735.jpg\n38246.jpg\n44504.jpg\n16916.jpg\n52876.jpg\n74055.jpg\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ✅ Corrected image directories\ntrain_img_dir = \"/kaggle/input/fairface/fairface/train\"\nval_img_dir = \"/kaggle/input/fairface/fairface/val\"\n\n# ✅ Fix img_path columns accordingly\ndf_train['img_path'] = df_train['file'].apply(lambda x: os.path.join(train_img_dir, os.path.basename(x)))\ndf_val['img_path'] = df_val['file'].apply(lambda x: os.path.join(val_img_dir, os.path.basename(x)))\n\n# Double-check\ndf_train[['file', 'img_path']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T11:59:55.757461Z","iopub.execute_input":"2025-07-16T11:59:55.757723Z","iopub.status.idle":"2025-07-16T11:59:55.836263Z","shell.execute_reply.started":"2025-07-16T11:59:55.757708Z","shell.execute_reply":"2025-07-16T11:59:55.835558Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"           file                                      img_path\n0   train/2.jpg   /kaggle/input/fairface/fairface/train/2.jpg\n1   train/3.jpg   /kaggle/input/fairface/fairface/train/3.jpg\n2   train/7.jpg   /kaggle/input/fairface/fairface/train/7.jpg\n3  train/10.jpg  /kaggle/input/fairface/fairface/train/10.jpg\n4  train/12.jpg  /kaggle/input/fairface/fairface/train/12.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train/2.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/2.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train/3.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/3.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train/7.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/7.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train/10.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/10.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train/12.jpg</td>\n      <td>/kaggle/input/fairface/fairface/train/12.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\n\n# 📐 Resize target\nIMG_SIZE = 64\n\n# 🧹 Load and preprocess images\ndef load_data_clean(df, img_size):\n    images = []\n    labels = []\n    skipped = 0\n\n    for i in range(len(df)):\n        img_path = df.iloc[i]['img_path']\n        label = df.iloc[i]['race_encoded']\n\n        try:\n            img = cv2.imread(img_path)\n            if img is None:\n                skipped += 1\n                continue\n            img = cv2.resize(img, (img_size, img_size))\n            img = img / 255.0  # normalize to [0,1]\n            images.append(img)\n            labels.append(label)\n        except:\n            skipped += 1\n            continue\n\n    X = np.array(images)\n    y = to_categorical(np.array(labels), num_classes=7)\n\n    print(f\"✅ Loaded: {len(images)} images | ❌ Skipped: {skipped} images\")\n    return X, y\n\n# 🧠 Load train & validation data\nX_train, y_train = load_data_clean(df_train, IMG_SIZE)\nX_val, y_val = load_data_clean(df_val, IMG_SIZE)\n\n# 📊 Show final shapes\nprint(\"✅ X_train shape:\", X_train.shape)\nprint(\"✅ y_train shape:\", y_train.shape)\nprint(\"✅ X_val shape:\", X_val.shape)\nprint(\"✅ y_val shape:\", y_val.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:00:32.425230Z","iopub.execute_input":"2025-07-16T12:00:32.425839Z","iopub.status.idle":"2025-07-16T12:06:29.730197Z","shell.execute_reply.started":"2025-07-16T12:00:32.425813Z","shell.execute_reply":"2025-07-16T12:06:29.729215Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded: 46492 images | ❌ Skipped: 0 images\n✅ Loaded: 5792 images | ❌ Skipped: 0 images\n✅ X_train shape: (46492, 64, 64, 3)\n✅ y_train shape: (46492, 7)\n✅ X_val shape: (5792, 64, 64, 3)\n✅ y_val shape: (5792, 7)\n","output_type":"stream"}],"execution_count":34}]}